---
# Prerequisites:
# $ pip install Pysphere
# mkisofs / on Mac via brew install cdrtools
# qemu-img
# ofvtool from vmware (this is a free tool)

# preparations for host to be provisioned happen on local machine - das sind im Prinzip alle tasks...!
- block:
  - include_tasks: "tools.yml"

  - name: Wait for vsphere to be available
    wait_for:
      host: '{{ vcenter_hostname }}'
      port: 80
      state: started
      timeout: 10

  - name: get facts from vsphere for guest (ignore failure)
    vmware_guest_facts:
      hostname: "{{ vcenter_hostname }}"
      username: "{{ vcenter_user }}"
      password: "{{ vcenter_pass }}"
      datacenter: ha-datacenter
      validate_certs: no
      name: "{{ inventory_hostname }}"
    register: guestfacts
    ignore_errors: true

  - name: fail if guest virtual machine already exists
    fail:
      msg: "A virtual machine on this host with the name {{ inventory_hostname }} already exists!"
    when:
      - guestfacts is successful
      - guestfacts.ansible_facts.hw_instance_uuid is regex("^[a-z,0-9]{8}\-.*$")

  - name: Temp default temp folder to "/var/tmp/{{ role_path|basename }}/"
    set_fact:
      vmproim_tempfolder: "/var/tmp/{{ role_path|basename }}/"
    tags:
      - provision

  - name: Temp folder on MacOS or if TMPDIR variable is found
    set_fact:
      vmproim_tempfolder: "{{ lookup('env','TMPDIR') }}{{ role_path|basename }}/"
    when: lookup('env','TMPDIR')
    tags:
      - provision

  - name: "create temp folder for staging of setup of vm images at {{ vmproim_tempfolder }}"
    file:
      path: "{{ vmproim_tempfolder }}"
      state: directory
      mode: 0755

  #- name: info temp folder will be deleted after run
  - debug: msg="info temp folder will be deleted after run"
    when: vmproim_delete_temp_folder == "yes"
    changed_when: true
    notify:
      - delete temp folder

  - name: create cloud init folder
    file:
      path: "{{ vmproim_tempfolder }}{{ inventory_hostname }}"
      state: directory
      mode: 0755
    tags:
      - provision

  - name: read this users ssh public key for cloud-init if no user has been set
    shell: cat ~/.ssh/id_rsa.pub
    become: no
    register: user_ssh_public_key
    when: vmproim_firstusersshpubkey == ""

  - name: set vmproim_firstusersshpubkey variable
    set_fact:
      vmproim_firstusersshpubkey: "{{ user_ssh_public_key.stdout }}"
    when: vmproim_firstusersshpubkey == ""

  - name: read this users username (running the deploy) if no user has been set
    local_action: command whoami
    become: no
    register: username_on_the_ansible_host
    when: vmproim_firstusername == ""

  - name: set vmproim_firstusername variable
    set_fact:
      vmproim_firstusername: "{{ username_on_the_ansible_host.stdout }}"
    when: vmproim_firstusername == ""

  - name: set vmproim_firstuserpasshash variable
    set_fact:
      vmproim_firstuserpasshash: 'sajEeYaHYyeSU' # 12345
    when: vmproim_firstuserpasshash == ""

  - name: "copy cloud-init user-data template for vm with initial user {{ vmproim_firstusername }}"
    template:
      src: user-data.txt.j2
      dest: "{{ vmproim_tempfolder }}{{ inventory_hostname }}/user-data"
    register: userdata_task

  - name: copy cloud-init meta-data template for vm
    template:
      src: meta-data.txt.j2
      dest: "{{ vmproim_tempfolder }}{{ inventory_hostname }}/meta-data"
    register: metadata_task

  # userdata via cloud-config openstack - funktioniert leider nicht in der vm.
  #- file:
  #    path: "{{ vmproim_tempfolder }}{{ inventory_hostname }}/openstack/latest"
  #    state: directory
  #    recurse: yes
  #- name: copy cloud-init userdata openstack template for vm
  #  template:
  #    src: user-data.txt.j2
  #    dest: "{{ vmproim_tempfolder }}{{ inventory_hostname }}/openstack/latest/user-data"

  - name: create cd image for cloud-init (with mkisofs)
    shell: "mkisofs -output {{ cloudinit_iso }} -r -J -V cidata {{ inventory_hostname }}/"
    args:
      chdir: "{{ vmproim_tempfolder }}"
    #shell: "hdiutil makehybrid -iso -o {{ cloudinit_iso }} {{ inventory_hostname }}/"
    # auskommentiert: -V user-data // -V cidata
      #creates: "{{ cloudinit_iso }}"
    #when: metadata_task.changed or userdata_task.changed
    #ansible_os_family == 'Darwin'

  - name: download hdd image file for vm (can take very long!)
    get_url:
      url: "{{ vmproim_imagesource }}"
      dest: "{{ vmproim_tempfolder }}{{ vmproim_img_filename }}"
      checksum: "sha256:{{ vmproim_img_sha256 }}"
    when: vmproim_imagesource is regex("^(http|ftp)s?.*")

  - name: copy hdd image for vm from given path
    copy:
      src: "{{ vmproim_imagesource }}"
      dest: "{{ vmproim_tempfolder }}{{ vmproim_img_filename }}"
    when: vmproim_imagesource is regex("^\/.*\/?.*")

  - stat:
      path: "{{ vmproim_tempfolder }}{{ vmproim_img_filename }}"
      checksum_algorithm: sha256
    register: vmproim_img_filename_stat
    when: vmproim_imagesource is regex("^\/.*\/?.*")

  - name: fail if checksum of copied image from path does not match
    fail:
      msg: "Checksum of the copied image source file differs from the requested checksum!"
    when:
      - vmproim_img_sha256 != vmproim_img_filename_stat.stat.checksum
      - vmproim_imagesource is regex("^\/.*\/?.*")

  - name: convert to vmdk (if neccessary)
    shell: "qemu-img convert -f {{ vmproim_img_filename | regex_replace('(^.*\\.)(.+)$', '\\2') }} -O vmdk -o adapter_type=lsilogic,subformat=streamOptimized,compat6=on -p {{ vmproim_img_filename }} {{ vmproim_vmdk }} > qemu-img-convert.log"
    #shell: "qemu-img convert {{ vmproim_img_filename }} -f {{ vmproim_img_filename | regex_replace('(^.*\\.)(.+)$', '\\2') }} -O vmdk -o adapter_type=lsilogic -p {{ vmproim_vmdk }}"
    args:
      chdir: "{{ vmproim_tempfolder }}"
      creates: "{{ vmproim_vmdk }}"
    register: convertvmdk
    # hint: on Mac and APFS filesystem qemu-img convert does not produce results that are usable up to now! Workaround: convert the file on an exFAT volume instead.

  - name: copy generic ovf template to local staging directory
    template:
      src: ovf-template.ovf.j2
      dest: "{{ vmproim_tempfolder }}preconverted_{{ ofvtemplate_name }}"
    register: ofvtemplate

  - name: convert VM including hdd image for VMWare with ofvtool
    shell: >
      ovftool
      --acceptAllEulas
      --noSSLVerify
      --skipManifestCheck
      --diskMode=thin
      --overwrite
      "preconverted_{{ ofvtemplate_name }}"
      "{{ ofvtemplate_name }}"
    args:
     chdir: "{{ vmproim_tempfolder }}"
    when: ofvtemplate.changed or convertvmdk.changed

  # run this whole block via:
  delegate_to: localhost
  tags:
    - prepare

- block:
  - name: deploy vm to VMWare host using VMWare ovftool
    shell: >
      ovftool
      --acceptAllEulas
      --noSSLVerify
      --skipManifestCheck
      --X:logFile="{{ vmproim_tempfolder }}deploy_ovftool_{{ inventory_hostname }}.log"
      --diskMode=thin
      --diskSize:template2,11="{{ disk * 1024 * 1024 }}"
      --name="{{ inventory_hostname }}"
      --datastore="{{ vmproim_datastore }}"
      --network="{{ vmproim_network }}"
      "{{ vmproim_tempfolder }}{{ ofvtemplate_name }}"
      "vi://{{ vcenter_user | urlencode }}:{{ vcenter_pass | urlencode }}@{{ vcenter_hostname }}:443"
    register: deploy_ovf_result
    # use --overwrite option if you want to overwrite the vm on the host,
    #       else it fails if the vm already exists

  # it seems not to work to deploy per module for free ESXi
  #- name: deploy vm to VMWare host
  #  vmware_deploy_ovf:
  #    hostname: "{{ vcenter_hostname }}"
  #    username: "{{ vcenter_user }}"
  #    password: "{{ vcenter_pass }}"
  #    datastore: "{{ vmproim_datastore }}"
  #    #folder:
  #    allow_duplicates: no
  #    name: "{{ inventory_hostname }}"
  #    networks: "{u'VM Network':u'{{ vmproim_network }}'}"
  #    validate_certs: no
  #    power_on: no
  #    ovf: "{{ vmproim_tempfolder }}{{ ofvtemplate_name }}"

  # the following task still doesnt work with the module
  #- name: "resize disk if needed on the VM on vSphere"
  #  vmware_guest_disk:
  #    hostname: "{{ vcenter_hostname }}"
  #    username: "{{ vcenter_user }}"
  #    password: "{{ vcenter_pass }}"
  #    validate_certs: no
  #    datacenter: "{{ vmproim_datacenter }}"
  #    folder: "{{ vmproim_datacenter }}/{{ vmproim_datastore }}/{{ inventory_hostname }}"
  #    name: "{{ inventory_hostname }}"
  #    disk:
  #      - size_gb: "{{ disk }}"
  #        datastore: "{{ vmproim_datastore }}"
  #        #type: thin
  #        state: present
  #        scsi_controller: 0
  #        unit_number: 0
  #        scsi_type: 'paravirtual'

  # run this whole block via:
  delegate_to: localhost
  tags:
    - provision

- block:
  # the following doesnt work on free ESXi
  #- name: power on vm on vSphere
  #  vmware_guest_powerstate:
  #    hostname: "{{ vcenter_hostname }}"
  #    username: "{{ vcenter_user }}"
  #    password: "{{ vcenter_pass }}"
  #    validate_certs: no
  #    folder: "{{ vmproim_datacenter }}/{{ vmproim_datastore }}/{{ inventory_hostname }}"
  #    name: "{{ inventory_hostname }}"
  #    state: powered-on

  - name: power on vm on vSphere
    vsphere_guest:
      vcenter_hostname: "{{ vcenter_hostname }}"
      username: "{{ vcenter_user }}"
      password: "{{ vcenter_pass }}"
      validate_certs: no
      guest: "{{ inventory_hostname }}"
      state: powered_on
      esxi:
        datacenter: "{{ vmproim_datacenter }}"
        hostname: "{{ vcenter_esxi }}"

  - name: set ip or hostname to ping
    set_fact:
      ping_hostname: "{% if ipv4 | ipv4 %}{{ ipv4 }}{% else %}{{ inventory_hostname }}{% endif %}"

  - name: "Wait for the post-install SSH to become available at {{ ping_hostname }}"
    wait_for:
      host: '{{ ping_hostname }}'
      port: 22
      state: started
      timeout: 300

  - name: wait for the vm to work init...
    pause:
      seconds: 20

  - name: Wait for the host to become unavailable = finished init
    wait_for:
      host: '{{ ping_hostname }}'
      port: 22
      state: stopped
      timeout: 300

  # warten, falls der host einige Zeit zum herunterfahren benötigt
  - pause:
      seconds: 10

  # the following task seem to work not as advertised
  - name: reconfigure VM on vSphere - detach cd image
    vmware_guest:
      hostname: "{{ vcenter_hostname }}"
      username: "{{ vcenter_user }}"
      password: "{{ vcenter_pass }}"
      datacenter: "{{ vmproim_datacenter }}"
      validate_certs: no
      name: "{{ inventory_hostname }}"
      state: poweredoff
      cdrom:
        type: none

  - name: power on vm on vSphere after detaching iso
    vsphere_guest:
      vcenter_hostname: "{{ vcenter_hostname }}"
      username: "{{ vcenter_user }}"
      password: "{{ vcenter_pass }}"
      validate_certs: no
      guest: "{{ inventory_hostname }}"
      state: powered_on
      esxi:
        datacenter: "{{ vmproim_datacenter }}"
        hostname: "{{ vcenter_esxi }}"

  - name: Wait for the post-reboot SSH to become available
    wait_for:
      host: '{{ ping_hostname }}'
      port: 22
      state: started
      timeout: 300
      sleep: 2

  # workaround: wait a long time because the the host will regenerate ssh keys after beeing startet up again so the first key that would be used would not be used afterwards by itself thus producing the problem of not being able to login...

  # via https://github.com/ansible-provisioning/ansible-provisioning/blob/master/README.md
  ### Revoke any existing host keys for this server (should become a separate ansible module to avoid conflicts)
  - name: Revoke any existing host keys for this server by hostname
    command: ssh-keygen -R {{ inventory_hostname_short }}

  # herausfinden der IP zum entfernen dieser aus den known_hosts
  - command: "dig +short {{ inventory_hostname }}"
    changed_when: false
    register: dig_output

  - set_fact:
      looked_up_ips: "{{ dig_output.stdout_lines }}"

  - debug: msg="found ip {{ item }}"
    with_items:
      - "{{ looked_up_ips }}"
      - "{{ ping_hostname }}"

  - name: Revoke any existing host keys for this server by ip address
    command: "ssh-keygen -R {{ item }}"
    with_items:
      - "{{ looked_up_ips }}"
      - "{{ ping_hostname }}"

  - name: ensure the newly created host is an ssh known host (unsecure for MITM!!)
    known_hosts:
      #path: /etc/ssh/ssh_known_hosts
      name: "{{ ping_hostname }}"
      key: "{{ lookup('pipe', 'ssh-keyscan -t rsa {{ ping_hostname }}') }}"

  # run this whole block via:
  delegate_to: localhost
  become: no
  tags:
    - reconfigure
